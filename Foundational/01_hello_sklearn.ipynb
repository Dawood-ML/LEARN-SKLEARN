{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b8ecfaa",
   "metadata": {},
   "source": [
    "# Muhammad Dawood Khan  \n",
    "\n",
    "### Welcome to the **Best Place to Learn scikit-learn (SKLEARN)**  \n",
    "Get ready for an **exciting journey** through Machine Learning with hands-on, practical examples.  \n",
    "Buckle up, explore, and most importantly — **enjoy the ride!**  \n",
    "\n",
    "---\n",
    "\n",
    "### Happy Coding & Keep Learning!\n",
    "If you have any **questions**, **feedback**, or **suggestions**, feel free to reach out anytime.  \n",
    "I’m always happy to connect with fellow learners and developers!\n",
    "\n",
    "---\n",
    "\n",
    "### Connect With Me  \n",
    "[![GitHub](https://img.shields.io/github/followers/Dawood-ML?label=GitHub&style=social)](https://github.com/Dawood-ML)  \n",
    "[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/muhammad-dawood-khan-5a3292329/)\n",
    "\n",
    "---\n",
    "\n",
    "_“Learning never exhausts the mind — it only sharpens it.”_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1708eb3",
   "metadata": {},
   "source": [
    "### **Chunk 1: The Scikit-Learn API Philosophy**\n",
    "\n",
    "#### **1. Concept Introduction**\n",
    "\n",
    "You understand the theory behind machine learning models. Scikit-learn's genius is not in the algorithms themselves, but in its **unified API**. Every algorithm, whether it's a simple regression or a complex ensemble, is an \"Estimator\" object that shares the same simple, consistent methods. This is the key to productivity.\n",
    "\n",
    "The three core methods are:\n",
    "\n",
    "1.  `.fit(X, y)`: This is the **training** step. The estimator \"learns\" from your data `X` (features) and `y` (target). Every supervised learning model in scikit-learn has this method.\n",
    "2.  `.predict(X_new)`: Once the model is trained, this method generates predictions for new, unseen data `X_new`.\n",
    "3.  `.transform(X)`: This is for **preprocessing**. It takes data and returns a transformed version of it (e.g., scaled, encoded, or with imputed values). We will cover this more in the next chunk. `fit_transform()` is a convenient shortcut that learns the transformation parameters from the data and applies the transformation in one step.\n",
    "\n",
    "This leads to the most fundamental workflow in all of supervised machine learning, which you will type thousands of times:\n",
    "\n",
    "**Load Data → Split Data → Train Model → Predict → Evaluate**\n",
    "\n",
    "Let's see this in action with our first dataset.\n",
    "\n",
    "#### **2. Dataset EDA: The Iris Dataset**\n",
    "\n",
    "The Iris dataset is the \"hello, world\" of machine learning. It's a clean, simple dataset for classifying three species of iris flowers based on four measurements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4be1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Set Plot style\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d200d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "# Create a Pandas DataFrame for easier manipulation\n",
    "df = pd.DataFrame(data=iris.data, \n",
    "                    columns=iris.feature_names)\n",
    "\n",
    "# Create target column\n",
    "df['target'] = iris.target\n",
    "df['species'] = df['target'].map({i: name for i, name in enumerate(iris.target_names)})\n",
    "\n",
    "print(\"INFO\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7412f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incase you are wondering what this line does.\n",
    "# Uncomment the code below\n",
    "\n",
    "#{i: name for i, name in enumerate(iris.target_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d4007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48316f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ea9a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"MIssing Values\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bda6375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variable Distribution\n",
    "print(\"Target Variable Distribution\")\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.countplot(x='species', data = df)\n",
    "plt.title(\"Distribution of Iris Species\")\n",
    "plt.xlabel('Species')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d19ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Distributions ( Histogram )\n",
    "df[iris.feature_names].hist()\n",
    "plt.suptitle('Histogram of Feature Distribution');\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc17f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Distributions ( Box PLots )\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.boxplot(data=df[iris.feature_names])\n",
    "plt.title('Box Plots of Features')\n",
    "plt.show(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c37800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot to visualize relationships\n",
    "# This is a powerful command to see realtionships between all pairs of features\n",
    "# and the distribution of each feature, colored by the target variable.\n",
    "sns.pairplot(df, hue = 'species', height=2.5)\n",
    "plt.suptitle('Pair Plot of Iris Dataset', y=1.02);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb3017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matric Heatmap\n",
    "# Select Only numeric features or columns for correlation\n",
    "df_num = df.drop(columns=['species'])\n",
    "plt.figure(figsize=(10,8))\n",
    "corr_mat  = df_num.corr() # Built in function in pandas\n",
    "sns.heatmap(corr_mat,\n",
    "            annot=True,\n",
    "            cmap='coolwarm',\n",
    "            linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Features and Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf4bd1b",
   "metadata": {},
   "source": [
    "## Minimal Working Example : \n",
    "> Load -> Split -> Train -> Predict -> Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6492da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMports\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# For reproducible results\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f93bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# We have already loaded the data, So there is no need to re-load it\n",
    "\n",
    "# Let's split the data into Features and Label\n",
    "#                OR\n",
    "#                                  X and  y\n",
    "X = df.drop(['target', 'species'], axis=1)\n",
    "y = df['target']\n",
    "\n",
    "print(F\"Features Shape : {X.shape}\")\n",
    "print(F\"Target Shape : {y.shape}\")\n",
    "\n",
    "# Another way of doing is : \n",
    "# Less common\n",
    "X, y  = load_iris(return_X_y=True)\n",
    "\n",
    "print(F\"Features Shape : {X.shape}\")\n",
    "print(F\"Target Shape : {y.shape}\")\n",
    "print(\"\\nSee Same thing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b4789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split DATA\n",
    "# Split the data into Training ( 80% ) and testing ( 20% ) sets.\n",
    "# stratify = y ensures that the proportion of classes in the train and test sets\n",
    "# is the same as the original dataset.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y\n",
    "                                                    )\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcde5469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Mode\n",
    "# 1. Instantiate the estimator (create the model object)\n",
    "# we'll use Logistic Regression, a simple and powerfull classification model\n",
    "model = LogisticRegression(max_iter=200) # INcreased max_iter for convergence\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "print(\"model training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ee669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "# Use the trained model to make predictions on the unseen data.\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"First 5 predictions : {y_pred[:5]}\")\n",
    "print(f\"First 5 actual labels : {y_test[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d55f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {100 * accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d09ae55",
   "metadata": {},
   "source": [
    "## Wow that is a nice model. Isn't it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ffb61d",
   "metadata": {},
   "source": [
    "## Variations & Key Concepts\n",
    "The beauty of Sklearn is its consistency. Let's swap the model. Notice how little the code changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7591085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Instantiate a new model\n",
    "new_model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Fit the model OR Train the model\n",
    "new_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_new  = new_model.predict(X_test)\n",
    "\n",
    "# Evaluate \n",
    "accuracy_knn = accuracy_score(y_test, y_pred_new)\n",
    "print(f\"Accuracy: {100 * accuracy_knn:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b02625c",
   "metadata": {},
   "source": [
    "Many models can also predict probabilities, which is often more useful than a hard prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Probabilities\n",
    "# Get the probabilities for each class for the first 5 test samples\n",
    "probabilities = model.predict_proba(X_test[:5])\n",
    "\n",
    "print(\"Probabilities for first 5 samples : \")\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print(probabilities)\n",
    "\n",
    "print(\"\\nPredicted class for first sample:\", np.argmax(probabilities[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b225c26f",
   "metadata": {},
   "source": [
    "# Common Pitfalls and Mistakes : \n",
    "    1. **Training and Testing on the Same Data (Data Leakage):** This is the #1 mistake. It gives you a perfect completely useless score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fafe9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT DO THIS\n",
    "model_leaky = LogisticRegression(max_iter=200)\n",
    "model_leaky.fit(X, y) # Fit on ALL data\n",
    "\n",
    "leaky_preds = model_leaky.predict(X) # Predict on the SAME data\n",
    "\n",
    "leaky_accuracy = accuracy_score(y, leaky_preds)\n",
    "print(f\"Dangerously optimistic accuracy: {leaky_accuracy * 100:.2f}%\") # This is meaningless!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7bb1af",
   "metadata": {},
   "source": [
    "    2. **Forgetting to INstantiate the Model:** You must create an *instance* of the model class by using parenthesis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04fd133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong way\n",
    "try:\n",
    "    LogisticRegression.fit(X_train, y_train)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Correct way\n",
    "model = LogisticRegression(max_iter=200) # Correct: creates an instance\n",
    "model.fit(X_train, y_train)\n",
    "print(\"\\nModel fitted correctly after instantiation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f4a38d",
   "metadata": {},
   "source": [
    "#### Congratulations. You have just successfully trained, predicted with, and evaluated your first two machine learning models using the standard, professional workflow. You've seen the core scikit-learn API (`fit`, `predict`) and used the most critical function for model validation (`train_test_split`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55380b92",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
