{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80aad4a7",
   "metadata": {},
   "source": [
    "### **Chunk 2: Data Splitting & Basic Preprocessing**\n",
    "\n",
    "#### **1. Concept Introduction**\n",
    "\n",
    "-   **Data Splitting Revisited**: `train_test_split` is great for a quick validation, but a single split can be lucky or unlucky. The gold standard is **cross-validation**, where we split the data into multiple \"folds\" and train/test the model multiple times. This gives a much more robust estimate of model performance. We'll dive deep into this later, but for now, know that `train_test_split` is our primary tool for fast iteration.\n",
    "\n",
    "-   **Why Preprocessing Matters**: Imagine you're building a model to predict house prices, and you have two features: `number_of_rooms` (from 1 to 10) and `square_footage` (from 500 to 5000). An algorithm that uses distance (like K-Nearest Neighbors) will be completely dominated by `square_footage`. A change of 100 sq ft will seem vastly more important than a change of 2 rooms, even if that isn't true. **Feature scaling** fixes this by putting all features on a common scale.\n",
    "\n",
    "-   **Core Preprocessing Tools**:\n",
    "    1.  `StandardScaler`: The most common scaler. It transforms each feature to have a mean of 0 and a standard deviation of 1. It assumes your data is normally distributed (a Gaussian distribution) and is not sensitive to outliers.\n",
    "    2.  `MinMaxScaler`: Scales features to a fixed range, usually 0 to 1. It's useful when you need bounded values or if the data is not normally distributed. It can be sensitive to outliers.\n",
    "    3.  `SimpleImputer`: Real-world data is often missing values (represented as `NaN`). You can't feed `NaN`s to a model. `SimpleImputer` is a basic strategy to fill them in, for example, with the mean, median, or most frequent value of the column.\n",
    "\n",
    "The most critical rule of preprocessing is: **You must learn the scaling parameters (like the mean and standard deviation) from the training data ONLY.** Applying `.fit()` to the entire dataset before splitting is a form of **data leakage** and will give you an artificially optimistic performance estimate.\n",
    "\n",
    "#### **2. Dataset EDA: The Wine Quality Dataset**\n",
    "\n",
    "This dataset contains chemical properties of red wines and a quality score from 3 to 8. It's great for our purpose because the features are all numeric but have very different scales. Our goal will be to classify a wine as \"good\" (quality > 5) or \"bad\" (quality <= 5).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d64941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad65b26",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51e07cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the Wine Wuality dataset form OPenML\n",
    "wine = fetch_openml(name='wine-quality-red',\n",
    "            version='active',\n",
    "            as_frame=True,\n",
    "            parser='auto')\n",
    "df = wine.frame\n",
    "\n",
    "# Dataset info\n",
    "print(\"Dataset info\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d97734",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6358feb4",
   "metadata": {},
   "source": [
    "**Create Target Variable**\n",
    "\n",
    "For this example, let's make it a binary classification problem.\n",
    "Quality > 5 is 'good' ( 1 ), otherwise 'bad' ( 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f15eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = pd.to_numeric(df['class'], errors='coerce')  # convert to numeric\n",
    "df['quality_binary'] = (df['class'] > 5).astype(int)\n",
    "df = df.drop('class', axis=1) # Drop the original multiclass target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45c5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c21c563",
   "metadata": {},
   "source": [
    "**Basic Statistics**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1affbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2e31e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64544c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60178a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Varibale Distribution\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='quality_binary', data=df)\n",
    "plt.title(\"Distribution of Wine Quality (0=bad, 1=Good)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00234159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Distribution ( Histogram ) \n",
    "# This clearly shows the different scales and distributions of each feature\n",
    "df.drop('quality_binary', axis=1).hist(figsize=(15, 12),\n",
    "bins=30,\n",
    "edgecolor='black')\n",
    "plt.suptitle('Histogram of Feature Distributions', y=0.92)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d5eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix Heatmap\n",
    "plt.figure(figsize=(12,10))\n",
    "crlm = df.corr()\n",
    "sns.heatmap(crlm, annot=True, cmap='viridis', fmt='.2f')\n",
    "plt.title('Correlation Matrix of Wine Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9967a76f",
   "metadata": {},
   "source": [
    "**3. Minimal Working Example: The Power of Scaling**\n",
    "\n",
    "Let's see the dramatic effect of scaling on a K-Nearest Neighbors (KNN) model, which is highly sensitive to feature scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3157485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, Data, Splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop('quality_binary', axis=1)\n",
    "y = df['quality_binary']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559a7612",
   "metadata": {},
   "source": [
    "**Attemp 1: Without Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c76f2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_raw = KNeighborsClassifier(n_neighbors=5) # 5 are default. THis was just to show that it exists. like you can change it if you want\n",
    "knn_raw.fit(X_train, y_train)\n",
    "y_pred_raw = knn_raw.predict(X_test)\n",
    "accuracy_raw = accuracy_score(y_pred=y_pred_raw, y_true=y_test)\n",
    "\n",
    "print(f\"KNN Accuracy WITHOUT Scaling: {accuracy_raw * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1ed8e",
   "metadata": {},
   "source": [
    "**Attempt 2: With Proper Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b9313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the scaler ON THE TRAINING DATA ONLY\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled  = scaler.fit_transform(X_train) # Fit and transform on train set\n",
    "\n",
    "# tranform the test data using the FITTED scaler\n",
    "# We must only use .transform() here to prevent data leakage from the test set.\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train and evaluate a new KNN model on the SCALED DATA\n",
    "knn_normal = KNeighborsClassifier()\n",
    "knn_normal.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = knn_normal.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "print(f\"KNN Accuracy WITH Scaling: {accuracy_scaled * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67595bb",
   "metadata": {},
   "source": [
    "See just by doing this one extra step we boosted the accuracy to 74%.\n",
    "\n",
    "This is why scaling is not optional but essential "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1208aa",
   "metadata": {},
   "source": [
    "**Variations :** `MinMaxScaler` and `SimpleImputer`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1d8439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max = MinMaxScaler()\n",
    "X_train_mm = min_max.fit_transform(X_train)\n",
    "X_test_mm  = min_max.transform(X_test)\n",
    "\n",
    "print(\"First 5 rows of MinMaxScaler transformed data : \")\n",
    "X_train_mm[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aa63ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SimpleImputer\n",
    "# Let's artificially add some missing values to demonstrate\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X_train_nan = X_train.copy()\n",
    "# Set 10% of values in the 'PH' column to NaN\n",
    "nan_indices  =  np.random.choice(X_train_nan.index,\n",
    "                                 size=int(len(X_train_nan)*0.1),\n",
    "                                 replace=False)\n",
    "X_train_nan.loc[nan_indices, 'pH'] = np.nan\n",
    "\n",
    "print(f\"Missing pH values bfore Imputation : {X_train_nan['pH'].isnull().sum()}\")\n",
    "\n",
    "# 1. Create imputer to fill with the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# 2. Fit on the Training Data and Transform it\n",
    "X_train_impu = imputer.fit_transform(X_train_nan)\n",
    "\n",
    "# 3. We would then use this simple Imputer to transform the test set\n",
    "X_test_impu = imputer.transform(X_test)\n",
    "\n",
    "print(f\"Missing values after imputation (on a new array): {np.isnan(X_train_impu).sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b0839f",
   "metadata": {},
   "source": [
    "## 5. Common Pitfalls\n",
    "**1. Leaking Data With the Scaler (CRITICAL MISTAKE) :** Never, ever fit your scaler on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30fd026",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_leaker = StandardScaler()\n",
    "X_leaky = scaler_leaker.fit_transform(X) # Fit on train AND Test data\n",
    "X_train_l, X_test_l, y_train_l, y_test_l = train_test_split(X_leaky, y)\n",
    "\n",
    "# Results will be overly optimistic and will not generalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3168d4b6",
   "metadata": {},
   "source": [
    "**2. Fitting the Scaler on the Test Set :** A more subtle but equally wrong mistake. This also leaks information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f0dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_wrong = StandardScaler()\n",
    "X_train_w    = scaler_wrong.fit_transform(X_train),\n",
    "# This re-learns the mean/std from the test set, which is wrong.\n",
    "X_test_w = scaler.fit_transform(X_test)\n",
    "print(\"Never use fit transform on test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0824aa",
   "metadata": {},
   "source": [
    "**3. Forgetting to Scale New Data :** When you deploy your model, any new, single prediction instance must also be scaled using the `scaler` you saved from your training process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b86fe95",
   "metadata": {},
   "source": [
    "### Congrats on Writing the code by hand. It takes dedication and self control to not copy paste code. You earned your right to move onto Chunk 3: First Supervised Learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2daed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
