{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2150808",
   "metadata": {},
   "source": [
    "### **Chunk 5: Essential Tree-Based Models**\n",
    "\n",
    "#### **1. Concept Introduction**\n",
    "\n",
    "Linear models try to find a single best line or plane to separate data. Tree-based models work differently. They learn by asking a series of simple \"yes/no\" questions to split the data into smaller and smaller groups.\n",
    "\n",
    "1.  **`DecisionTreeClassifier`/`DecisionTreeRegressor`**:\n",
    "    -   **How it works**: Imagine a flowchart. The model starts with all your data at the top (the \"root\"). It then finds the best possible feature and split-point (e.g., \"Is `median_income` <= 3.5?\") that divides the data into two purer groups (the \"branches\"). It repeats this process on each new group, creating a tree structure. To make a prediction for a new data point, it simply follows the path down the tree from the root to a final \"leaf\" node and returns the majority class or average value of the training samples that ended up in that leaf.\n",
    "    -   **Pros**: Extremely easy to understand and visualize. The decision-making process is transparent.\n",
    "    -   **Cons**: Their biggest weakness is a tendency to **overfit**. A single decision tree can keep splitting until every single data point is in its own leaf, perfectly memorizing the training data but failing to generalize to new data.\n",
    "\n",
    "2.  **`RandomForestClassifier`/`RandomForestRegressor`**:\n",
    "    -   **How it works**: This is an **ensemble** model that brilliantly solves the overfitting problem of a single decision tree. Instead of building one giant, complex tree, it builds hundreds of smaller, simpler trees (a \"forest\").\n",
    "    -   The \"Random\" part is key:\n",
    "        1.  Each tree is trained on a different random sample of the training data (a bootstrap sample).\n",
    "        2.  At each split point, each tree only considers a random subset of the features.\n",
    "    -   This randomness ensures that the individual trees are diverse and each makes slightly different errors. To make a final prediction, the Random Forest averages the predictions of all the individual trees (for regression) or takes a majority vote (for classification). This \"wisdom of the crowd\" approach dramatically reduces overfitting and leads to a much more robust and accurate model.\n",
    "    -   **Feature Importance**: A fantastic byproduct of tree-based models is the ability to easily calculate feature importance. The model can track how much each feature contributes to reducing impurity (making better splits) across all the trees in the forest. This gives you a powerful tool to understand which features are driving the predictions.\n",
    "\n",
    "#### **2. Dataset EDA: California Housing Dataset**\n",
    "\n",
    "The goal is to predict the median house value for California districts, based on features from the 1990 census. This is a classic regression task perfect for demonstrating the power of tree-based models and their feature importance capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23d7d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2304d153",
   "metadata": {},
   "source": [
    "$Load$ $Data$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2019a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing  = fetch_california_housing()\n",
    "df = pd.DataFrame(data=housing.data,\n",
    "                  columns=housing.feature_names)\n",
    "df['MedHouseVal'] = housing.target\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5860dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198e2dc6",
   "metadata": {},
   "source": [
    "`Basic Statisitcs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26693cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c4933",
   "metadata": {},
   "source": [
    "`Check for missing values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dfc245",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80822dd",
   "metadata": {},
   "source": [
    "`Target Variable Distribution (Median House Value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3788e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['MedHouseVal'], bins=50,\n",
    "             kde=True)\n",
    "plt.title('Distribution of Median House Value in California')\n",
    "plt.xlabel('Median House Value ($100,000s)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eed297",
   "metadata": {},
   "source": [
    "***Correlation Matrix Heatmap***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c3356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "corr_mat  = df.corr()\n",
    "sns.heatmap(corr_mat,\n",
    "            annot=True,\n",
    "            cmap='YlGnBu',\n",
    "            fmt='.2f')\n",
    "plt.title('Correlation Matrix of California housing Features')\n",
    "plt.show()\n",
    "# 'MedInc' (Median Income) has the strongest positive correlation with the target.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b64b9e",
   "metadata": {},
   "source": [
    "#### **3. Minimal Working Example: Overfitting and the Random Forest Solution**\n",
    "\n",
    "One of the best things about tree models is that they are **not sensitive to feature scaling**. The splitting decisions don't depend on the magnitude of the features, so we can skip the `StandardScaler` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e8753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports, Data, and Splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990158b8",
   "metadata": {},
   "source": [
    "**Attempt 1: A single decision Tree (demonstrating overfitting)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a7cbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an unconstrained Decision Tree\n",
    "# By not setting max_depth, the tree can grow as deep as it wants\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097800e7",
   "metadata": {},
   "source": [
    "**Evaluate the Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6863a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tree  = tree_reg.predict(X_test)\n",
    "rmse_tree    = np.sqrt(mean_squared_error(y_test, y_pred_tree))\n",
    "\n",
    "# Let's also check the performance on the Training set\n",
    "y_train_pred_tree = tree_reg.predict(X_train)\n",
    "rmse_train_tree   = np.sqrt(mean_squared_error(y_true=y_train, y_pred=y_train_pred_tree))\n",
    "\n",
    "print(f\"Decision Tree TRAIN RMSE : {rmse_train_tree:.4f}\")\n",
    "print(f\"Decision Tree TEST RMSE: {rmse_tree:.4f}\")\n",
    "# The TRAIN RMSE is 0.0! This is a classic sign of massive overfitting.\n",
    "# The model has perfectly memorized the training data but performs poorly on new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc51290a",
   "metadata": {},
   "source": [
    "**Attempt 2: This time with Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c14b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest Regressor\n",
    "# n_estimators is the number of trees in the forest.\n",
    "# n_jobs = -1 uses all available CPU cores for faster training.\n",
    "forest_reg = RandomForestRegressor(n_estimators=100,\n",
    "                                   random_state=42,\n",
    "                                   n_jobs=-1)\n",
    "forest_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40b0341",
   "metadata": {},
   "source": [
    "**Evaluate the Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a719dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_forest = forest_reg.predict(X_test)\n",
    "rmse_forest = np.sqrt(mean_squared_error(y_test, y_pred_forest))\n",
    "\n",
    "# Check performance on the TRAINING set again\n",
    "y_train_pred_forest = forest_reg.predict(X_train)\n",
    "rmse_train_forest = np.sqrt(mean_squared_error(y_train, y_train_pred_forest))\n",
    "\n",
    "print(f\"Random Forest TRAIN RMSE: {rmse_train_forest:.4f}\")\n",
    "print(f\"Random Forest TEST RMSE:  {rmse_forest:.4f}\")\n",
    "# Notice two things:\n",
    "# 1. The TEST RMSE is significantly better than the single decision tree.\n",
    "# 2. The gap between TRAIN and TEST RMSE is much, much smaller. This is a well-generalized model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809cc635",
   "metadata": {},
   "source": [
    "#### **4. Feature Importance Extraction**\n",
    "Now for the best part. Let's see what the Random Forest learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c1d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imps = forest_reg.feature_importances_\n",
    "feature_names = housing.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e2c07a",
   "metadata": {},
   "source": [
    "Create a Pandas DataFrame for easier visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c90cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "featu_imp_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importances': imps\n",
    "}).sort_values('importances', ascending=False)\n",
    "featu_imp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724b6c04",
   "metadata": {},
   "source": [
    "##### PLotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5555102",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importances', y='feature', data=featu_imp_df)\n",
    "plt.title('Feature Importance for California Housing Prediction')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "# As the EDA suggested, Median Income is by far the most important feature.\n",
    "# The geographical coordinates (Latitude, Longitude) are also highly important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b225b2",
   "metadata": {},
   "source": [
    "#### **5. Common Pitfalls**\n",
    "\n",
    "1.  **Shipping an Overfit Decision Tree**: Never use a single, unconstrained `DecisionTree` as your final model. It's a great tool for learning, but `RandomForest` is almost always superior for prediction.\n",
    "2.  **Choosing `n_estimators`**: More trees (`n_estimators`) is generally better, but it comes at the cost of longer training time and memory usage. Performance usually plateaus after a few hundred trees. 100 is a very common and effective starting point.\n",
    "3.  **Ignoring Hyperparameters**: We used the defaults here, but tuning parameters like `max_depth` (how deep each tree can be) and `min_samples_leaf` (minimum samples required to be at a leaf node) is crucial for getting the best performance. We will cover this with `GridSearchCV` soon.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699d4761",
   "metadata": {},
   "source": [
    "# Move on to Chunk 06 Pipeplines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c492d242",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
