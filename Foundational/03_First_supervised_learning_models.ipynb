{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be90c6ef",
   "metadata": {},
   "source": [
    "### **Chunk 3: First Supervised Learning Models**\n",
    "\n",
    "#### **1. Concept Introduction**\n",
    "\n",
    "We are now moving into the core of supervised learning. You will learn the two workhorse models that form the basis for many more advanced techniques:\n",
    "\n",
    "1.  **`LogisticRegression` for Classification**: Despite its name, Logistic Regression is a **classification** algorithm. It works by calculating the probability that a given input belongs to a certain class. From a mathematical standpoint, it fits a line (or a plane in higher dimensions) that best separates the different classes in your data. It's fast, highly interpretable, and a fantastic baseline model.\n",
    "\n",
    "2.  **`LinearRegression` for Regression**: This is the classic statistical model for **regression** tasks (predicting a continuous value like price or temperature). It finds the best-fitting linear relationship between the features and the target. The goal is to find the coefficients (weights) for each feature that minimize the difference between the predicted and actual values.\n",
    "\n",
    "A key advantage of both models is their **interpretability**. After training, you can inspect the model's learned `coef_` attribute. These coefficients tell you how much a one-unit increase in a feature affects the prediction, holding all other features constant. A large positive coefficient means the feature strongly increases the probability/value of the target, while a large negative coefficient means it strongly decreases it.\n",
    "\n",
    "#### **2. Dataset EDA: Breast Cancer Wisconsin Dataset (Classification)**\n",
    "\n",
    "This is another classic, clean dataset from `sklearn.datasets`. The goal is to predict whether a breast tumor is malignant (cancerous) or benign (not cancerous) based on 30 numeric features computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e90c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deecb11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "cancer  = load_breast_cancer()\n",
    "df      = pd.DataFrame(data = cancer.data,\n",
    "                       columns=cancer.feature_names)\n",
    "df['target'] = cancer.target\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c51b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Statistics\n",
    "# Notice the different scales again (e:g., 'mean area' vs 'mean smoothness')\n",
    "pd.set_option('display.max_columns', None) # Show all columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d557bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5270ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variable Distribution\n",
    "print(df['target'].value_counts())\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='target', data=df)\n",
    "plt.title('Distribution of Target (0=Malignant, 1=Benign)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fec6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Distributions (Histograms)\n",
    "# Let's look at the 'mean' features for brevity\n",
    "mean_feature = [col for col in df.columns if 'mean' in col]\n",
    "df[mean_feature].hist(figsize=(15, 12),\n",
    "                      bins=30, edgecolor='black')\n",
    "plt.suptitle('Histograms of \"Mean\" Feature Distributions', y=0.92)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28be936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matric Heatmap\n",
    "plt.figure(figsize=(20, 15))\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=False,\n",
    "            cmap='viridis') # Annot=False due to high number of features\n",
    "plt.title('Correlation Matrix of Breast Cancer Features') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81123f0b",
   "metadata": {},
   "source": [
    "**3. Minimal Working Example (Classification)**\n",
    "\n",
    "Let's build a `LogisticRegression` model. We'll follow the exact same pattern as before:\n",
    "**Split --> Scale --> Train --> Predict --> Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfde8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, Data, Splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.linear_model    import LogisticRegression\n",
    "from sklearn.metrics         import accuracy_score\n",
    "\n",
    "X,y = cancer.data, cancer.target\n",
    "\n",
    "X_train, X_test, y_train, y_test  = \\\n",
    "                    train_test_split(\n",
    "                        X,y,\n",
    "                        test_size=0.2,\n",
    "                        random_state=42,\n",
    "                        stratify=y\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00025ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCale the Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled  =  scaler.fit_transform(X_train)\n",
    "X_test_scaled   =  scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a98b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train \n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb7a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_pred=y_pred,\n",
    "                          y_true=y_test)\n",
    "print(f\"Logistic Regression Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5817aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpreting Coefficients\n",
    "# Create a DataFrame to view the coefficients alongside their feature names\n",
    "coefficients = pd.DataFrame(\n",
    "    data=log_reg.coef_.T, # Transpose to make it a column\n",
    "     index=cancer.feature_names,\n",
    "      columns=['coefficient']  \n",
    ")\n",
    "\n",
    "# Sort by the absolute value to see the most impactful features\n",
    "coefficients['abs_coefficient'] = coefficients['coefficient'].abs()\n",
    "coefficients = coefficients.sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "coefficients.head()\n",
    "\n",
    "# NEGATIVE COEFFICIENT -> increases chance of bgein class 0 (Malignant)\n",
    "# POSITIVE COEFFICIENT -> increases chance of being class 1 (Benign)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aacc915",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5ce79e",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbf3e28",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42446a35",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0e3d3d",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961d5a4a",
   "metadata": {},
   "source": [
    "**4. Dataset EDA: Boston Housing Dataset (Regression)**\n",
    "The goal is to predict the median value of owner-occupied homes (MEDV) in the Boston area using various features about the suburbs.\n",
    "\n",
    "**NOTE:** The original `load_boston` function in scikit-learn is depracated due to ethical concerns with the dataset. We will fetch a version from OpenML, which is the modern way to access many classic datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6913cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9756715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = fetch_openml(name=\"boston\",\n",
    "                      version=1,\n",
    "                      as_frame=True,\n",
    "                      parser='auto')\n",
    "df = boston.frame\n",
    "# The Target variable is named 'MEDV' in this version\n",
    "df['target'] = boston.target\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ff3d73",
   "metadata": {},
   "source": [
    "**Basic INfo of the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff08e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4751a838",
   "metadata": {},
   "source": [
    "**Basic Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db27e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc868610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variable Distribution\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(df['target'],\n",
    "             kde=True,\n",
    "             bins=30)\n",
    "plt.title('Distribution of House Prices (MEDV)')\n",
    "plt.xlabel('Median Value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46811ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with target\n",
    "# Find features most correlated with the target price\n",
    "corr_with_target  =  df.corr()['target'].sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr_with_target.drop('target').plot(kind='bar')\n",
    "plt.title('Correlation of Features with house Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44000b89",
   "metadata": {},
   "source": [
    "5. **Minimal Working Example (Regression)**\n",
    "\n",
    "Let's build a `LinearRegression` model. The workflow is identical, just the model and evaluation metric change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74602c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Use .values to get numpy arrays\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values\n",
    "print(X.shape, y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(f\"X_Train shape : {X_train.shape}\")\n",
    "print(f\"X_Test shape : {X_test.shape}\")\n",
    "print(f\"y_Train shape : {y_train.shape}\")\n",
    "print(f\"y_Test shape : {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2d422",
   "metadata": {},
   "source": [
    "**Scale The Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb148be",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448e460",
   "metadata": {},
   "source": [
    "**Train the Linear Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce27dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ca38d",
   "metadata": {},
   "source": [
    "**Predict and Evaluate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e834b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lin_reg.predict(X_test_scaled)\n",
    "\n",
    "# We'll use Root Mean Squared Error (RMSE), which is in the same units as the target.\n",
    "mse  =   mean_squared_error(y_test, y_pred)\n",
    "rmse  = np.sqrt(mse)\n",
    "print(f\"Linear Regression RMSE: ${rmse * 1000:.2f}\") # Multiply by 1000 since target is in $1000s\n",
    "\n",
    "# OR we can also do\n",
    "rmse = np.sqrt(np.mean(np.square(y_test - y_pred)))\n",
    "print(f\"Linear Regression RMSE: ${rmse * 1000:.2f}\")\n",
    "\n",
    "# OR we can also do\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "rmse = root_mean_squared_error(y_true=y_test, y_pred=y_pred)\n",
    "print(f\"Linear Regression RMSE: ${rmse * 1000:.2f}\")\n",
    "\n",
    "# the last one is probably the best . BUt I wanted to open your mind a little towards possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e2208b",
   "metadata": {},
   "source": [
    "**Interpreting Coefficients**\n",
    "> Creating a DataFrame to view the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ee5399",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = df.drop('target', axis=1).columns\n",
    "coeffs = pd.DataFrame(\n",
    "    data=lin_reg.coef_,\n",
    "    index = feature_names,\n",
    "    columns=['coefficient']\n",
    ").sort_values('coefficient', ascending=False)\n",
    "print('Feature Coefficients :')\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f126c0",
   "metadata": {},
   "source": [
    "Coefficients indicate how much **each feature affects the house price (MEDV)**, assuming all others remain constant.\n",
    "\n",
    "---\n",
    "\n",
    "## **Features Explained (with Interpretation of Coefficients)**\n",
    "\n",
    "| Feature     | Meaning                                                    | Coefficient      | Interpretation                                                                                                                                                                                                                                 |\n",
    "| ----------- | ---------------------------------------------------------- | ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **MEDV**    | Dependent variable — Median home value ($1000s)            | **9.388523e+00** | This is likely your **intercept** (bias term). It means that when all other features are 0, the model predicts a median value of **≈ $9,388**. (In real life, this number alone doesn’t have meaning since features can’t realistically be 0.) |\n",
    "| **RAD**     | Index of accessibility to radial highways                  | **1.15e-14**     | Practically **0**, meaning highway access has **no strong linear relationship** with home price in this fitted model.                                                                                                                          |\n",
    "| **AGE**     | Proportion of owner-occupied units built before 1940       | **1.03e-14**     | Again **almost zero** — older homes don’t seem to linearly affect prices here (maybe due to feature scaling or correlation).                                                                                                                   |\n",
    "| **INDUS**   | Proportion of non-retail business acres per town           | **7.66e-15**     | **Very small positive effect** — but practically negligible; more industrial area doesn’t meaningfully affect price in this model.                                                                                                             |\n",
    "| **LSTAT**   | % of lower-status population                               | **6.99e-15**     | Normally, this should be **negative** (higher LSTAT → lower price), so your result near **0** means the model didn’t capture the expected inverse relationship (possibly due to normalization, regularization, or numerical precision).        |\n",
    "| **TAX**     | Property tax rate per $10,000                              | **6.33e-15**     | No meaningful linear relationship captured here — higher taxes usually decrease prices, but your coefficient is almost 0.                                                                                                                      |\n",
    "| **CRIM**    | Per capita crime rate by town                              | **5.65e-15**     | Nearly 0 — meaning crime rate didn’t show a measurable linear effect on price in this fit.                                                                                                                                                     |\n",
    "| **PTRATIO** | Pupil–teacher ratio by town                                | **4.99e-15**     | Practically 0 — education quality didn’t show an effect here, but usually it does (lower PTRATIO → higher prices).                                                                                                                             |\n",
    "| **NOX**     | Nitric oxide concentration (pollution level)               | **1.11e-15**     | Close to 0 — no linear relation captured, though typically higher NOX → lower price.                                                                                                                                                           |\n",
    "| **CHAS**    | Charles River dummy variable (1 if tract bounds river)     | **6.80e-16**     | No noticeable effect, though in the real dataset, being near the river usually **increases** prices.                                                                                                                                           |\n",
    "| **B**       | Proportion of Black population (historical, coded feature) | **-3.99e-15**    | Essentially 0 — model didn’t learn any effect. (Note: this feature is outdated and ethically inappropriate in modern datasets.)                                                                                                                |\n",
    "| **RM**      | Average number of rooms per dwelling                       | **-4.16e-15**    | Surprisingly **slightly negative**, but magnitude is near 0. Normally, more rooms → higher price.                                                                                                                                              |\n",
    "| **DIS**     | Weighted distances to employment centers                   | **-6.44e-15**    | Nearly 0 — distance from work centers didn’t matter linearly here.                                                                                                                                                                             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911d8e99",
   "metadata": {},
   "source": [
    "#### **6. Common Pitfalls**\n",
    "\n",
    "1.  **Interpreting Coefficients on Unscaled Data**: If you don't scale your features first, a feature with a large scale (like `DIS` in Boston) might get a tiny coefficient, while a feature with a small scale gets a huge one, completely misleading your interpretation of their importance. **Always interpret coefficients on scaled data.**\n",
    "2.  **Assuming Causation**: The coefficients show **correlation**, not **causation**. A high negative coefficient for `LSTAT` (lower status of population) doesn't *cause* low prices on its own; it's correlated with a complex mix of factors that lead to lower prices.\n",
    "3.  **Using the Wrong Model for the Task**: Trying to use `LinearRegression` on the Breast Cancer (classification) dataset will result in an error or nonsensical predictions. The inverse is also true.\n",
    "\n",
    "#### **7. Quick Win**\n",
    "\n",
    "Incredible progress. You have now successfully built, evaluated, and interpreted models for the two primary types of supervised machine learning problems. You can:\n",
    "-   Solve a classification problem with `LogisticRegression`.\n",
    "-   Solve a regression problem with `LinearRegression`.\n",
    "-   Scale data to ensure model performance and valid interpretation.\n",
    "-   Inspect model coefficients (`.coef_`) to understand what drives the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10333f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Use .values to get numpy arrays\n",
    "X_new = df.drop('target', axis=1).values\n",
    "y_new = df['target'].values\n",
    "print(X_new.shape, y_new.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.2, random_state=42)\n",
    "print(f\"X_Train shape : {X_train.shape}\")\n",
    "print(f\"X_Test shape : {X_test.shape}\")\n",
    "print(f\"y_Train shape : {y_train.shape}\")\n",
    "print(f\"y_Test shape : {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e88221",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_on_not_Scaled_data = LinearRegression()\n",
    "lin_reg_on_not_Scaled_data.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lin_reg_on_not_Scaled_data.predict(X_test)\n",
    "\n",
    "# We'll use Root Mean Squared Error (RMSE), which is in the same units as the target.\n",
    "mse  =   mean_squared_error(y_test, y_pred)\n",
    "rmse  = np.sqrt(mse)\n",
    "print(f\"Linear Regression on not scaled data RMSE: ${rmse * 1000:.2f}\") # Multiply by 1000 since target is in $1000s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5550e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = df.drop('target', axis=1).columns\n",
    "coeffs_not_scaled = pd.DataFrame(\n",
    "    data=lin_reg_on_not_Scaled_data.coef_,\n",
    "    index = feature_names,\n",
    "    columns=['coefficient']\n",
    ").sort_values('coefficient', ascending=False)\n",
    "print('Feature Coefficients :')\n",
    "coeffs_not_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "mse  =   mean_squared_error(y_test, y_pred)\n",
    "rmse  = np.sqrt(mse)\n",
    "print(f\"Random Forest RMSE: ${rmse * 1000:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911d262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_forest = pd.DataFrame(\n",
    "    data=rf.feature_importances_,\n",
    "    index = feature_names,\n",
    "    columns=['coefficient']\n",
    ").sort_values('coefficient', ascending=False)\n",
    "print('Feature Coefficients :')\n",
    "coeffs_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c20fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
