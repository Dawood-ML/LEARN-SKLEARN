{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9ed080",
   "metadata": {},
   "source": [
    "### **Chunk 4: Model Evaluation Fundamentals**\n",
    "\n",
    "#### **1. Concept Introduction**\n",
    "\n",
    "The choice of metric depends entirely on the problem you are trying to solve and the cost of different types of errors.\n",
    "\n",
    "**For Classification:**\n",
    "\n",
    "Imagine you are building a model to detect a rare disease.\n",
    "-   **False Positive (FP):** The model predicts a healthy person has the disease. This causes stress but is resolved with further testing.\n",
    "-   **False Negative (FN):** The model predicts a sick person is healthy. This is a catastrophic failure.\n",
    "\n",
    "In this case, minimizing False Negatives is far more important than minimizing False Positives. `accuracy` treats all errors equally, which is why it's often a misleading metric.\n",
    "\n",
    "Here are the key metrics built from the **Confusion Matrix**:\n",
    "-   **Precision**: \"Of all the times the model predicted 'Positive', how often was it correct?\" Use this when the cost of a **False Positive** is high. (e.g., sending a promotional offer to a customer who won't respond).\n",
    "    -   `Precision = TP / (TP + FP)`\n",
    "-   **Recall (Sensitivity)**: \"Of all the actual 'Positive' cases, how many did the model find?\" Use this when the cost of a **False Negative** is high. (e.g., fraud detection, medical diagnosis).\n",
    "    -   `Recall = TP / (TP + FN)`\n",
    "-   **F1-Score**: The harmonic mean of Precision and Recall. It provides a single score that balances both concerns. It's a great default metric if you care about both FPs and FNs.\n",
    "-   **Confusion Matrix**: A table that visualizes the performance, showing you exactly where your model is getting confused (e.g., consistently misclassifying the digit '8' as a '3').\n",
    "\n",
    "**For Regression:**\n",
    "\n",
    "-   **Mean Absolute Error (MAE)**: The average absolute difference between the predicted and actual values. It's easy to interpret because it's in the same units as the target.\n",
    "-   **Mean Squared Error (MSE)**: The average of the squared differences. The squaring penalizes larger errors much more heavily than smaller ones. Its units are squared, making it less intuitive.\n",
    "-   **Root Mean Squared Error (RMSE)**: The square root of MSE. This brings the metric back to the same units as the target, making it interpretable while still punishing large errors. It's the most common regression metric.\n",
    "-   **R-squared (R²)**: The coefficient of determination. It represents the proportion of the variance in the target variable that is predictable from the features. A score of 1.0 means the model explains all the variability; 0 means it explains none.\n",
    "\n",
    "#### **2. Dataset EDA: The Digits Dataset**\n",
    "\n",
    "This dataset contains images of handwritten digits (0-9). Each image is 8x8 pixels, flattened into a 64-feature vector. It's a classic multi-class classification problem where a confusion matrix is essential to see which specific digits the model struggles with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d492ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3957b6",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e145374",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "print(f\"Features shape : {X.shape}\")\n",
    "print(f\"Target SHape : {y.shape}\")\n",
    "print(f\"NUmber of unique classes : {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9054ec",
   "metadata": {},
   "source": [
    "**Visualize the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a1175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5),\n",
    "                       subplot_kw={'xticks':[],\n",
    "                                    'yticks':[]})\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X[i].reshape(8, 8),\n",
    "              cmap='binary',\n",
    "              interpolation='nearest')\n",
    "    ax.set_title(f\"True Label: {y[i]}\")\n",
    "\n",
    "plt.suptitle(\"Sample IMages from the Digits Dataset\", fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a421d3f0",
   "metadata": {},
   "source": [
    "**Target Variable Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=y)\n",
    "plt.title('Distribution of Digits')\n",
    "plt.xlabel('Digit')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "# The dataset is very well-balanced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8316c07d",
   "metadata": {},
   "source": [
    "##### **3. Minimal Working Example (Classification Metrics)**\n",
    "\n",
    "Let's train a model and then dive into the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4473a040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMports, Splitting, Scaling, and Training\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Split the data\n",
    "X_train,\\\n",
    "X_test,\\\n",
    "y_train,\\\n",
    "y_test = \\\n",
    "        train_test_split(\n",
    "            X, \n",
    "            y,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=y\n",
    "        )\n",
    "print(f\"Shape of X_Train : {X_train.shape}\")\n",
    "print(f\"Shape of X_Test : {X_test.shape}\")\n",
    "print(f\"Shape of y_Train : {y_train.shape}\")\n",
    "print(f\"Shape of y_Test : {y_test.shape}\")\n",
    "print(f\"MINIMUM VALUE IN TRAINING BEFORE SCALING : {X_train.min()} \")\n",
    "print(f\"MAXIMUM VALUE IN TRAINING BEFORE SCALING : {X_train.max()} \")\n",
    "print()\n",
    "print(f\"MEAN IN TRAINING BEFORE SCALING : {np.mean(X_train)}\")\n",
    "print(f\"STD IN TRAINING BEFORE SCALING : {np.std(X_train)}\")\n",
    "print(\"The dataset already seems pretty solid but normalizing\\nwill always be a crucial part before modelling\")\n",
    "# SCale the Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "print()\n",
    "print(f\"MINIMUM VALUE AFTER SCALING : {X_train_scaled.min()} \")\n",
    "print(f\"MAXIMUM VALUE AFTER SCALING : {X_train_scaled.max()} \")\n",
    "print()\n",
    "print(f\"MEAN AFTER SCALING : {np.mean(X_train_scaled)}\")\n",
    "print(f\"STD AFTER SCALING : {np.std(X_train_scaled)}\")\n",
    "print()\n",
    "\n",
    "# Training the model\n",
    "model =  LogisticRegression(max_iter=1000,\n",
    "                            random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(f\"First 5 Predictions made by the model : {y_pred[:5]}\")\n",
    "print(f\"First 5 true labels :                   {y_test[:5]}\")\n",
    "print(\"Pretty close. Let's evaluate our model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420cf11d",
   "metadata": {},
   "source": [
    "**The CLassification report**\n",
    "* This is the single most useful evaluation function for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf52a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_pred, target_names=digits.target_names.astype(str))\n",
    "print('classification_report')\n",
    "print(report)\n",
    "# 'Support' is the number of true instances for each class in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b12dd0d",
   "metadata": {},
   "source": [
    "**The Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b62dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix\")\n",
    "print(cm)\n",
    "print(\"\\nSo beautiful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4aaf49",
   "metadata": {},
   "source": [
    "**visualizing the confusion Matrix**\n",
    "* A heatmap is much easier to interpret than the raw numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24bea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm,\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=digits.target_names,\n",
    "            yticklabels=digits.target_names\n",
    "            )\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True label')\n",
    "plt.title('Confusion Matrix for Digit Recognition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fa692e",
   "metadata": {},
   "source": [
    "#### 4. Variations (Regression Metrics)\n",
    "\n",
    "Let's quickly revisit the Boston Housing dataset from the previous chunk to demonstrate the regression metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab3aeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and train a regression model\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Load data\n",
    "boston = fetch_openml(name=\"boston\",\n",
    "                      version=1,\n",
    "                      as_frame=True,\n",
    "                      parser='auto')\n",
    "X_boston, y_boston = boston.data, boston.target\n",
    "\n",
    "# Split, Scale, Train\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_boston,\n",
    "                                                            y_boston,\n",
    "                                                            test_size=0.2,\n",
    "                                                            random_state=42)\n",
    "print(f\"Shape of X_Train : {X_train_b.shape}\")\n",
    "print(f\"Shape of X_Test : {X_test_b.shape}\")\n",
    "print(f\"Shape of y_Train : {y_train_b.shape}\")\n",
    "print(f\"Shape of y_Test : {y_test_b.shape}\")\n",
    "\n",
    "# scale\n",
    "scaler_b = StandardScaler()\n",
    "X_train_b_scaled = scaler_b.fit_transform(X_train_b)\n",
    "X_test_b_scaled  = scaler_b.transform(X_test_b)\n",
    "\n",
    "# model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_b_scaled, y_train_b)\n",
    "y_pred_b = lin_reg.predict(X_test_b_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4cd28e",
   "metadata": {},
   "source": [
    "#### $Calculate$ $and$ $Print$ $regression$ $metrics$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e17f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_test_b, y_pred_b)\n",
    "mse = mean_squared_error(y_test_b, y_pred_b)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_b, y_pred_b)\n",
    "\n",
    "print(\"Regression Metrics\")\n",
    "print(f\"Mean Absolute Error (MAE): ${mae*1000:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): ${rmse*1000:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c017125",
   "metadata": {},
   "source": [
    "#### **5. Common Pitfalls**\n",
    "\n",
    "1.  **Using Accuracy on Imbalanced Datasets:** As discussed, this is the biggest trap. If a dataset has 99% class A and 1% class B, a model that always predicts A has 99% accuracy but is completely useless. Always check your target distribution and use precision/recall/F1 for imbalanced problems.\n",
    "2.  **Looking at Only One Metric:** No single metric tells the whole story. R² can be high, but your RMSE might still be too large for your business case. Precision might be high, but recall could be terrible. Always look at a suite of metrics.\n",
    "3.  **Confusing the Axes of the Confusion Matrix:** Be very careful to label which axis is \"True\" and which is \"Predicted\". Scikit-learn's `confusion_matrix(y_true, y_pred)` puts `y_true` on the y-axis and `y_pred` on the x-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3101d4",
   "metadata": {},
   "source": [
    "# Congragulations Soldier. You may now proceed to Chunk 05 : Essential Tree-Based Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd20ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
