{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d309cb",
   "metadata": {},
   "source": [
    "### **Chunk 7: Cross-Validation & Hyperparameter Tuning**\n",
    "\n",
    "#### **1. Concept Introduction**\n",
    "\n",
    "-   **Hyperparameters vs. Parameters**: Model **parameters** are learned from the data during `.fit()` (e.g., the coefficients in `LinearRegression`). Model **hyperparameters** are settings you choose *before* training (e.g., the number of trees in a `RandomForest`).\n",
    "\n",
    "-   **The Problem with `train_test_split` for Tuning**: How do you pick the best hyperparameter? You could try several values, train each on `X_train`, and see which performs best on `X_test`. **This is a trap!** By using `X_test` to choose your hyperparameter, you have implicitly leaked information from the test set into your model selection process. Your final performance estimate will be overly optimistic. The test set must be kept in a vault, untouched until the very end.\n",
    "\n",
    "-   **Cross-Validation (CV)**: The solution is to create a *validation set* out of the training set. The most robust way to do this is **K-Fold Cross-Validation**.\n",
    "    1.  You split the *training data* into K equal-sized \"folds\" (e.g., K=5).\n",
    "    2.  You train your model on K-1 folds and evaluate it on the held-out fold.\n",
    "    3.  You repeat this K times, with each fold getting its turn as the validation set.\n",
    "    4.  The final performance is the average of the K scores. This gives a much more stable and reliable estimate of your model's performance on unseen data.\n",
    "    5.  **`StratifiedKFold`** is essential for classification. It ensures that the class distribution in each fold is the same as in the overall training set, which is critical for imbalanced datasets.\n",
    "\n",
    "-   **Hyperparameter Tuning Tools**:\n",
    "    1.  **`GridSearchCV`**: The brute-force method. You define a \"grid\" of hyperparameters you want to test (e.g., `n_estimators: [100, 200, 300]`, `max_depth: [5, 10, 15]`). It then uses cross-validation to exhaustively train and evaluate a model for *every single combination* of these parameters. It's thorough but can be very slow.\n",
    "    2.  **`RandomizedSearchCV`**: A smarter, faster alternative. Instead of trying every combination, it samples a fixed number of random combinations from the grid. This is often more efficient because not all hyperparameters are equally important.\n",
    "    3.  **Optuna (The Modern Standard)**: This is where we level up. Optuna is an advanced hyperparameter optimization framework that uses intelligent search strategies (like Bayesian optimization) to find the best parameters. Instead of searching blindly, it learns from past trials. If it sees that high values of a certain parameter are performing poorly, it will stop exploring that region and focus on more promising areas. This makes it vastly more efficient and effective than grid or random search. We will use Optuna as our primary tool.\n",
    "\n",
    "#### **2. Dataset EDA: Credit Approval Dataset**\n",
    "\n",
    "This dataset from the UCI repository concerns credit card applications. Due to confidentiality, all feature names have been anonymized (A1, A2, etc.). It's a great real-world example with mixed data types and a slightly imbalanced target, making it perfect for a tuned pipeline. The goal is to predict whether an application was approved (+) or rejected (-).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e57122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2251d4",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe39069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset has no header and uses `?` for missing values\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data'\n",
    "col_names = [f'A{i}' for i in range(1, 16)] + ['class']\n",
    "df = pd.read_csv(url, header = None, names=col_names,\n",
    "                 na_values='?')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc22eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccd2c7d",
   "metadata": {},
   "source": [
    "**Data Cleaning and Type conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e299d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class or 'y' label is object. Let's see it\n",
    "print(df['class'].head(3))\n",
    "print(f\"Unique values in class : {np.unique(df['class'])}\")\n",
    "print(\"\\nok great we can just convert this to 0 and 1\")\n",
    "df['class'] = (df['class'] == '+').astype(int) # Convert target to 0/1\n",
    "print(df['class'].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d72f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf647b86",
   "metadata": {},
   "source": [
    "**Missing Values check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59794ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7fc320",
   "metadata": {},
   "source": [
    "**Target Variable Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef626a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target Variable Distribution\")\n",
    "print(df['class'].value_counts(normalize=True))\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='class', data=df)\n",
    "plt.title('Distribution of credit approval (0: Rejected, 1: Approved)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d55555",
   "metadata": {},
   "source": [
    "A very slight imbalance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472382ef",
   "metadata": {},
   "source": [
    "\n",
    "##### **3. Minimal Working Example: From GridSearchCV to Optuna**\n",
    "\n",
    "We'll use a RandomForestClassifier and tune its hyperparameters. First, we'll set up the pipeline just like in the last chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6fa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup, imports, Data, splitting, and Pipeline\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = df.drop(['class'], axis=1)\n",
    "y = df['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)\n",
    "\n",
    "numeric_features     = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric Features {numeric_features}\")\n",
    "print(f\"Categorical Features {categorical_features}\")\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('inputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "    ])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# Now we will define the model seperately to attach it to different search methods\n",
    "model  = RandomForestClassifier(random_state=42,\n",
    "                                n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42edb02f",
   "metadata": {},
   "source": [
    "**Attempt : 1 : GridSearchCV (The Slow Way)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f249864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the Pipeline\n",
    "pipeline  = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "    ])\n",
    "\n",
    "# Define the parameter grid. Note the 'classifier__' prefic to target pipeline steps\n",
    "param_grid = {\n",
    "    'classifier__n_estimators':[50, 100, 200],\n",
    "    'classifier__max_depth': [5, 10, None],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Total fits = 3 * 3 * 3 * 5(CV folds) = 135 models!!!\n",
    "\n",
    "# Set up the search\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "grid_search = GridSearchCV(pipeline,\n",
    "                           param_grid=param_grid,\n",
    "                           cv = cv,\n",
    "                           scoring='accuracy',\n",
    "                           n_jobs=-1,\n",
    "                           verbose=1)\n",
    "\n",
    "# Run the search\n",
    "print(\"Running Grid search cv\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fac881",
   "metadata": {},
   "source": [
    "#### **Attempt 2: Optuna (The professional Way)**\n",
    "\n",
    "optuna requires you to define an \"objective\" function. This function takes a `trial` object, uses it to suggest hyperparameters, builds and evaluates the model, and returns the score you want to maximize or minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd29f9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define the optuna objective function\n",
    "def objective(trial):\n",
    "    # 1. define the hyperparmeter seach space\n",
    "    # The pipeline and preprocessorare fixed, we only tune the classifier.\n",
    "    classifier_name = trial.suggest_categorical('classifier', ['RandomForest']) # We can even test differnt models.\n",
    "    if classifier_name == 'RandomForest':\n",
    "        n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "        max_depth    = trial.suggest_int('max_depth', 3, 20)\n",
    "        min_sample_leaf  = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "\n",
    "        # Now we redefine the model inside the objhective with the suggested params\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_leaf=min_sample_leaf,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    # Create the full pipeline\n",
    "    pipeline = Pipeline(steps =[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "\n",
    "    # Evaluate the pipeline using cross-validation\n",
    "    # We use the same data and CV strategy as before for a fair comparison\n",
    "    cv = StratifiedKFold(n_splits=5,\n",
    "                         shuffle=True,\n",
    "                         random_state=42)\n",
    "    score= cross_val_score(\n",
    "        pipeline,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        n_jobs=-1,\n",
    "        cv= cv,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    accuracy = score.mean()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8bbaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Optuna study\n",
    "# create_study directs Optuna to maximize the objective function's return value.\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50) # Run for 50 trials (compare to 135 for GridSearch)\n",
    "\n",
    "print(\"\\n--- Optuna Results ---\")\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value (accuracy): {trial.value:.4f}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6adda8a",
   "metadata": {},
   "source": [
    "#### **4. Using the Best Model**\n",
    "\n",
    "Once you have the best hyperparameters from Optuna, you create the final model with them and train it on the entire training set. Then you do your final evaluation on the held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4142133",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4e863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best params and create final pipeline\n",
    "best_params = study.best_params\n",
    "# The 'classifier' param is from our objective function, we don't need it for the model itself\n",
    "best_params.pop('classifier') \n",
    "\n",
    "final_model = RandomForestClassifier(random_state=42, n_jobs=-1, **best_params)\n",
    "final_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', final_model)])\n",
    "\n",
    "# Train on the full training set\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the unseen test set\n",
    "test_accuracy = final_pipeline.score(X_test, y_test)\n",
    "print(f\"\\nFinal Test Set Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd5ffef",
   "metadata": {},
   "source": [
    "#### **5. Common Pitfalls**\n",
    "\n",
    "1.  **Tuning on the Test Set**: The cardinal sin, as discussed. Cross-validation on the training set is the only correct way.\n",
    "2.  **Using `KFold` for Classification**: Forgetting to use `StratifiedKFold` can give you unstable CV scores if one of your folds happens to get a skewed distribution of classes by random chance.\n",
    "3.  **Unrealistic Search Spaces**: Defining a `GridSearchCV` that would take days to run. Start small, see what regions look promising, and then refine your search. This is another reason Optuna is superiorâ€”it does this refining for you automatically.\n",
    "4.  **Not Setting `random_state`**: Forgetting to set a `random_state` in your model, CV splitter, and `train_test_split` makes your results non-reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70505a2",
   "metadata": {},
   "source": [
    "### Massive Effort. You have come a long way but there is still a long way to go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af989f7e",
   "metadata": {},
   "source": [
    "### Proceed to chunk 08: Handling Real-World Data Issues. You have earned it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
